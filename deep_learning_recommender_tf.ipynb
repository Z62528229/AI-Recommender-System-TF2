
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "ccda213f-debb-4fa1-9b72-8f10c112b3fe",
     "kernelId": ""
    },
    "id": "1zKrxVlfnqvM"
   },
   "source": [
    "# Deep Learning Recommenders in TensorFlow\n",
    "\n",
    "Dr. Nick Ball  \n",
    "Data-Scientist-in-Residence, Paperspace\n",
    "\n",
    "Last updated: Nov 12th 2021\n",
    "\n",
    "This self-contained notebook shows the use of Paperspace Gradient to implement a recommender system using TensorFlow. It accompanies the 6-part blog series \"Gradient End-to-End: A Recommender System using Notebooks and Workflows\" on the [Paperspace blog](https://blog.paperspace.com), and the associated [Git Repository](https://github.com/gradient-ai/Deep-Learning-Recommender-TF).\n",
    "\n",
    "The project includes these main highlights:\n",
    "\n",
    "1. Show a real-world-style example of machine learning on Gradient\n",
    "2. End-to-end dataflow incorporating both Gradient Notebooks and Workflows\n",
    "3. Modern data science methodology based on Gradient's integrations with Git\n",
    "4. Use TensorFlow 2 and TensorFlow Recommenders (TFRS) to train a recommender model that includes deep learning\n",
    "5. Use training data that reflects what real-world projects deal with (not just demo data)\n",
    "6. Construct a custom model using the full TensorFlow subclassing API\n",
    "7. Show working hyperparameter tuning that improves the results\n",
    "8. Deploy model using Gradient Deployments and its TensorFlow Serving integrations\n",
    "9. Accompanying material: self-contained working Jupyter notebook, and Git repository\n",
    "10. Aimed at a broad technical audience: data scientists who are not engineers, engineers who are not data scientists, those who span the two disciplines, and others\n",
    "\n",
    "The project is not a complete enterprise-grade recommender system: they would typically take teams of several people months to construct, and result in an amount of code far greater than shown here, but it aims to be more than just a simple demonstration or toy model, by showcasing real data science techniques. In the Appendix we discuss some of the steps one might take to go from the project here to a full system, with a focus on Gradient's capabilities.\n",
    "\n",
    "We assume the reader of this notebook is somewhat technical, but not necessarily an expert in recommender systems, deep learning, TensorFlow, or Paperspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "1c6661f1-13d8-4a0c-a567-2f68f08a5bdc",
     "kernelId": ""
    }
   },
   "source": [
    "## Note: Section 5 is coming soon!\n",
    "\n",
    "Model deployment support in the Gradient product on public clusters is currently pending, expected in Sept. 2021. Therefore section 5 of the Notebook on model deployment is shown but will not yet run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "b53f6968-20b4-499e-9cec-e491c4e37097",
     "kernelId": ""
    }
   },
   "source": [
    "### Requirements\n",
    "\n",
    "The notebook is designed to run on Paperspace Gradient\n",
    "\n",
    " - In the Gradient GUI, create a Project with a name, e.g., Deep Learning Recommender\n",
    " - Within the project, create a Notebook with the following settings:\n",
    "   - Don't select any of the boxes under Select a runtime\n",
    "   - Select a machine = C4 (C5, P4000, etc., will also work)\n",
    "   - Public/private = set to preferred option\n",
    "   - Under Advanced options:\n",
    "     - Set the Workspace URL field to https://github.com/gradient-ai/Deep-Learning-Recommender-TF to point to this repository.\n",
    "     - Set the Container name field to `tensorflow/tensorflow:2.4.1-gpu-jupyter`\n",
    "     - Set the Container command to `jupyter notebook --allow-root --ip=0.0.0.0 --no-browser --NotebookApp.trust_xheaders=True --NotebookApp.disable_check_xsrf=False --NotebookApp.allow_remote_access=True --NotebookApp.allow_origin='*'`\n",
    "     - The other options can remain the same.\n",
    " - Start the Notebook\n",
    "\n",
    "Once the Notebook has started, click `deep_learning_recommender_tf.ipynb` to run in the usual way by clicking Run under each cell in turn.\n",
    "\n",
    "Notebook creation can also be done on the command line if desired, via gradient notebooks create. For more details on Notebooks, see the documentation.\n",
    "\n",
    "Alternatively, you can clone the Git repository and run in your own notebook setup\n",
    "\n",
    " - `git clone https://github.com/gradient-ai/Deep-Learning-Recommender-TF.git` (the repo is public, so Git's ssh command form is not required)\n",
    " - Be able to run Python 3, and import modules as in Section 2 below: Matplotlib, NumPy, TensorFlow 2, TFDS, TFRS\n",
    " - We use some notebook cell magic lines, such as `%matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "5b638cdd-222a-4188-953b-e15a349b64a2",
     "kernelId": ""
    }
   },
   "source": [
    "### Additional requirements to run Gradient Workflows and model deployment (sections 4.3+)\n",
    "\n",
    "In addition to the above, to run the Gradient Workflows in section 4.3 and the model deployment in section 5 requires some more setup.\n",
    "\n",
    "This step is optional:\n",
    "\n",
    " - Use or create a [Gradient Private Cluster](https://docs.paperspace.com/gradient/gradient-private-cloud/about/setup/managed-installation) and [get its ID](https://docs.paperspace.com/gradient/gradient-private-cloud/about/usage#finding-your-cluster-id). If no cluster ID is specified, the Gradient public cluster will be used.\n",
    "\n",
    "These requirements will remain, being associated with Workflows as the enterprise-grade production part of Gradient:\n",
    "\n",
    " - [Create a project](https://docs.paperspace.com/gradient/get-started/managing-projects#create-a-project) and [get its ID](https://docs.paperspace.com/gradient/get-started/managing-projects#get-your-projects-id)\n",
    " - [Generate an API key](https://docs.paperspace.com/gradient/get-started/quick-start/install-the-cli#obtaining-an-api-key) for your project to allow access\n",
    " - [Store the API key](https://docs.paperspace.com/gradient/get-started/managing-projects/storing-an-api-key-as-a-secret) as a secret in your project\n",
    "\n",
    "These steps will go away as the product matures around Workflows:\n",
    "\n",
    " - [Create the two workflows](https://docs.paperspace.com/gradient/explore-train-deploy/workflows/getting-started-with-workflows#creating-gradient-workflows) named `recommender-train-model` and `recommender-deploy-model`, and [get their IDs](https://docs.paperspace.com/gradient/explore-train-deploy/workflows/getting-started-with-workflows#running-your-first-workflow-run)\n",
    " - [Create an output dataset](https://docs.paperspace.com/gradient/data/data-overview/private-datasets-repository#creating-a-dataset-and-dataset-version) for the training workflow, named `recommender`\n",
    " \n",
    "The create workflow step and create output dataset can be done via the GUI, or the [command line interface](https://docs.paperspace.com/gradient/get-started/quick-start/install-the-cli) (CLI). The commands for the latter look like\n",
    "\n",
    "`gradient storageProviders list --apiKey <your API key>` \n",
    " \n",
    "Note the `storage provider ID` for **Gradient Managed** storage.\n",
    "\n",
    "`gradient datasets create --name recommender --storageProviderId <your storage provider ID> --apiKey <your API key>`  \n",
    "`gradient workflows create --name Recommender-Train-Model --projectId <your project ID> --apiKey <your API key>`  \n",
    "`gradient workflows create --name Recommender-Deploy-Model --projectId <your project ID> --apiKey <your API key>`\n",
    "\n",
    "The usage of `--apiKey` on the command line is optional: you can also store a key in a JSON file, e.g., `~/.paperspace/config.json` to avoid having to add it to each command. Here we leave it present. Another option is the environment variable `PAPERSPACE_API_KEY`. See [connecting your account](https://docs.paperspace.com/gradient/get-started/quick-start/install-the-cli#connecting-your-account) for more details.\n",
    "\n",
    "Note that one thing that is *not* required for this project is any setup on your own machine, unless you choose to use the CLI. This is because the Workflows are invoked from this Notebook, via the SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "080fde46-2160-4aef-82f2-6cd9ceec9a92",
     "kernelId": ""
    }
   },
   "source": [
    "### Format of notebook text\n",
    "\n",
    "The main text is shown as MarkDown, like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "73b20b1b-b9c4-4454-a715-7068f7a48f81",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optional extra information is in cell comments, like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "17f2ea5b-5a7b-4590-b3d1-fb11f08d4754",
     "kernelId": ""
    }
   },
   "source": [
    "### Recommenders\n",
    "\n",
    "Recommender systems are widely used in modern artificial intelligence, most prominently in retail and entertainment content. However, many of these systems in businesses still use classical methods such as matrix factorization that may not capture all of the information now available.\n",
    "\n",
    "The addition of the fully nonlinear mappings allowed by machine learning, for example deep learning neural network layers, can improve the performance of recommender systems by both capturing more of the complex patterns of information that are present, and by making it easier to add new information in the form of further data feature columns, such as detailed text descriptions or reviews, multiple user and item information columns, or timestamps.\n",
    "\n",
    "Here, we use the new TensorFlow Recommenders library to show how the addition of deep learning to a recommender model improves its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "185f3b48-0dca-441c-8c34-7d5fd0a5e9ce",
     "kernelId": ""
    },
    "id": "VAsFUrkgyYAu"
   },
   "source": [
    "# Contents\n",
    "\n",
    "1. Introduction: Recommender systems and deep learning\n",
    "2. Setup\n",
    "3. Preparing the dataset\n",
    "4. Build the recommender models\n",
    "5. Deploy the final model\n",
    "6. Conclusions\n",
    "\n",
    "Next Steps  \n",
    "Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "f4b12273-0446-4da1-96cf-8868d3dd1d3f",
     "kernelId": ""
    },
    "id": "tnDTzpDQ_hUS"
   },
   "source": [
    "## 1: Introduction: Recommender systems and deep learning\n",
    "\n",
    "A good way to describe most recommender systems is:\n",
    "\n",
    "*Present suggested new items to a user that users similar to them already liked.*\n",
    "\n",
    "This is on the assumption that if users similar to them liked something, there is a better-than-random chance that they will like it too.\n",
    "\n",
    "The short sentence above involves several concepts, including information about a user, how to measure if they are similar to another user, what items the other users liked, how to choose candidate items to present from the whole list, and how to rank those choices.\n",
    "\n",
    "(There are further ideas as well, such as whether a user rated an item explicitly or just implicitly liked it by watching or buying it, and using other similarities such as item-item as well as user-user similarity.)\n",
    "\n",
    "Here, we use the well-known MovieLens dataset, containing information about which movies users watched, along with details of the movies, the users, and the times of viewing. The idea is to present to a user suggested movies that they might like to watch next.\n",
    "\n",
    "While not a huge modern dataset with millions of viewings from millions of users, it has been widely used, and is \"real enough\" to show both how deep learning can improve recommender models, and how to set up and end-to-end data science workflow.\n",
    "\n",
    "The content of this notebook is based on the tutorials of the [TensorFlow Recommenders (TFRS) library](https://www.tensorflow.org/recommenders), modified to include more real-world data science steps, and showing a model being deployed into production.\n",
    "\n",
    "As described there, recommender models commonly consist of two parts:\n",
    "\n",
    " - Retrieval, which selects possible candidates from the whole list of items that could be recommended, in this case being movies the user might be interested in\n",
    " - Ranking, which narrows down this list to a refined set of items to recommend\n",
    "\n",
    "We concentrate on the ranking portion, showing how the addition of deep learning layers, data features, and hyperparameter tuning, improves the performance of the model.\n",
    "\n",
    "We then show how the resulting model can be easily deployed with Gradient Deployments.\n",
    "\n",
    "The concentration on just one part of the models to be built is so that we can show more fully realized data science steps, the end-to-end process using both Gradient Notebooks and Workflows, and some of the corresponding functionaity, without the project becoming overly long or repetitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "cd8dc7dd-a9be-421a-a3ea-1414e9c6c096",
     "kernelId": ""
    },
    "id": "3EeRY5DQI5VV"
   },
   "source": [
    "## 2: Setup\n",
    "\n",
    "This project uses [TensorFlow Datasets (TFDS)](https://www.tensorflow.org/datasets) and the TensorFlow Recommenders library (TFRS) in addition to the basic TensorFlow 2 and Python 3.\n",
    "\n",
    "TFRS was created as a separate library because recommender systems typically do not correspond to the simple setup of supervised learning models, but have a more complex arrangement of processing steps associated with them. So rather than using a few layers in the high-level TensorFlow Keras Sequential or [Functional](https://www.tensorflow.org/guide/keras/functional) APIs, they need the custom model and custom layer setup in the lower level representation of writing the model classes and subclasses directly.\n",
    "\n",
    "However, recommenders do contain their own common components, such as the FactorizedTopK performance metric for retrieval, that mean usage of a library is considerably more convenient than writing something from scratch in TensorFlow.\n",
    "\n",
    "We use TensorFlow Datasets as the emphases in this project are recommenders, Gradient functionality, end-to-end, and showing some model tuning, rather than data gathering, cleaning, and preparation. Of course, in a full enterprise system this would be a more prominent aspect of the dataflow.\n",
    "\n",
    "Let's install TFDS, TFRS, and import these and the Python modules that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "dbe8232b-9934-495c-87cd-55f403afc2b2",
     "kernelId": ""
    },
    "id": "9gG3jLOGbaUv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 18.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.2.4\n",
      "    Uninstalling pip-20.2.4:\n",
      "      Successfully uninstalled pip-20.2.4\n",
      "Successfully installed pip-21.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Currently the notebook is using pip install, which would not be ideal in a production environment.\n",
    "# This is because the install changes the environment and the versions of libraries being installed are\n",
    "# not necessarily fixed, meaning the environment is not fixed and hence reproducibility is not guaranteed.\n",
    "# One could use requirements.txt, but this also does not necessarily fix things due to secondary dependencies.\n",
    "\n",
    "# Various solutions exist, but for rigorous work, Gradient allows the user to build a custom container\n",
    "# containing the correct dependencies, which removes the need for pip install.\n",
    "\n",
    "# Here, the container we use fixes versions, except for TFDS and TFRS, whose versions we therefore fix.\n",
    "\n",
    "!pip install --upgrade pip\n",
    "!pip install -q tensorflow-recommenders==0.4.0\n",
    "!pip install -q --upgrade tensorflow-datasets==4.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "951710e0-dcad-4a49-ad0e-ac4d3227671c",
     "kernelId": ""
    },
    "id": "SZGYDaF-m5wZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "# This is Python's function for more nicely or compactly printing variables or data\n",
    "import pprint\n",
    "\n",
    "# Python 3 allows hints on variable types to be given\n",
    "# Here they are used in the model class definitions\n",
    "from typing import Dict, Text\n",
    "\n",
    "# NumPy is used, e.g., for viewing the data in TensorFlow tensors\n",
    "# TensorFlow tensors are built on NumPy arrays\n",
    "import numpy as np\n",
    "\n",
    "# This allows simple inline plots of, e.g., model training history\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid') # Following TFRS tutorials\n",
    "\n",
    "# TensorFlow, TFDS, TFRS\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "351e3deb-5d51-4db2-a0e8-88e4e6501049",
     "kernelId": ""
    },
    "id": "ecPlfiYXJDQe"
   },
   "source": [
    "We can see some basic information about the versions of software that we are using. In principle, the container could be accessed to find out more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "id": "01073185-dfff-499a-ae9b-a123ac8f48b9",
     "kernelId": ""
    },
    "id": "Nc5Um6Y44D-Q",
    "outputId": "df8fe35b-e2e5-4a09-be4a-13c63c757ebe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.19.5\n",
      "Python version: 3.6.9\n",
      "TensorFlow version: 2.4.0\n",
      "TensorFlow Datasets version: 4.2.0\n",
      "TensorFlow Recommenders version: v0.4.0\n"
     ]
    }
   ],
   "source": [
    "print('NumPy version: {}'.format(np.__version__))\n",
    "print('Python version: {}'.format(platform.python_version()))\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "print('TensorFlow Datasets version: {}'.format(tfds.__version__))\n",
    "print('TensorFlow Recommenders version: {}'.format(tfrs.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "d9913690-51b7-479c-8b13-036b5d136f8e",
     "kernelId": ""
    },
    "id": "GJORoRytYAGI"
   },
   "source": [
    "We can see if GPUs, etc., are available, or just CPU. Here we do not currently specify devices other than the default, as the processing run is not large enough to require a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "id": "f8318f17-c47b-47c8-a50a-4f23abf8e99e",
     "kernelId": ""
    },
    "id": "o914Boo2YDYz",
    "outputId": "1c553eac-8a74-4955-990e-6a6245fc14da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11116880676765012419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "aca0175a-a3f1-4788-9cec-b9fe76a4a9c0",
     "kernelId": ""
    },
    "id": "5PAqjR4a1RR4"
   },
   "source": [
    "## 3: Preparing the dataset\n",
    "\n",
    "Load the MovieLens data from TensorFlow Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "01ab5e7e-104b-4139-8078-dcb3c0e30c7d",
     "kernelId": ""
    },
    "id": "aaQhqcLGP0jL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 4.70 MiB (download: 4.70 MiB, generated: 32.41 MiB, total: 37.10 MiB) to /root/tensorflow_datasets/movielens/100k-ratings/0.1.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3255d04c6ca949fba8dca13c54a5d952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c5859755424e0ea931c2a3ffd6e08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7949e5a980044e5db2b8698044520d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345c14be36064a8fbe548521780c9ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd95f5d1911415293a7d59af6b829ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c0b43291f94d4a8e4cae1e78224b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling movielens-train.tfrecord...:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset movielens downloaded and prepared to /root/tensorflow_datasets/movielens/100k-ratings/0.1.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# This loads the data from the official TensorFlow datasets repository\n",
    "# at https://www.tensorflow.org/datasets/catalog/movielens\n",
    "# The -ratings suffix indicates that we are loading the dataset with the\n",
    "# movies data joined to the ratings data\n",
    "\n",
    "ratings_raw = tfds.load('movielens/100k-ratings', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "id": "f68becde-80ae-40c5-a75a-1e3ba526f11f",
     "kernelId": ""
    },
    "id": "X_sx4C_Bw4sc",
    "outputId": "aff47275-f87a-482c-eb1f-a8093d449c97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: {bucketized_user_age: (), movie_genres: (None,), movie_id: (), movie_title: (), raw_user_age: (), timestamp: (), user_gender: (), user_id: (), user_occupation_label: (), user_occupation_text: (), user_rating: (), user_zip_code: ()}, types: {bucketized_user_age: tf.float32, movie_genres: tf.int64, movie_id: tf.string, movie_title: tf.string, raw_user_age: tf.float32, timestamp: tf.int64, user_gender: tf.bool, user_id: tf.string, user_occupation_label: tf.int64, user_occupation_text: tf.string, user_rating: tf.float32, user_zip_code: tf.string}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is loaded as a TensorFlow PrefetchDataset\n",
    "ratings_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "fd129db8-c8df-43cc-aabb-86e618e517c2",
     "kernelId": ""
    },
    "id": "9VReDbPf4QRU"
   },
   "source": [
    "Data science workflows, especially in the experimental and model training stage as here, should contain a strong component of viewing the actual data throughout the analysis. This is both for the understanding of the experimenter, other readers, and for sanity checking.\n",
    "\n",
    "Here we see that various columns (features) are available, including information about the user, the movie they watched, and outcomes such as the user's rating of the movie.\n",
    "\n",
    "It is important to distinguish between information that is available before a user watches a movie from that which is only available after, as the latter cannot be used as a feature for training a recommender model. This is because the data to be fed into it when it is deployed must be available before the user has watched the movie that is being recommended to them.\n",
    "\n",
    "In this case, the requirement manifests as the `user_rating` column being a training target and not a feature, and a timestamp column that should be normalized into something that is cyclical like a day of week / month / year, and not an absolute date. Such a date won't come around again in the future when the model has been deployed on unseen data.\n",
    "\n",
    "We also immediately see issues with the data that may manifest further along in the dataflow, such as some of the columns being byte-encoded (strings like `b'357'` instead of just `'357'`). If the setup of TensorFlow Serving plus the RESTful API is to be used for deployment, these will have to be converted to be able to be represented in JSON for passing to the model when it is deployed.\n",
    "\n",
    "The end-to-end mentality of having deployment in scope from the start, enabled by Gradient, has encouraged us to look for and notice these issues right here, rather than spending time training the model and only then noticing them. The latter approach could result in signficant wasted effort if it turns out that the deployment issues are not solvable.\n",
    "\n",
    "The top 2 rows of data are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "id": "4cdf7d0d-e4fc-4f1a-b6b4-ae18fac1b47b",
     "kernelId": ""
    },
    "id": "JdNMayvVzhP9",
    "outputId": "2790ba40-5156-4601-aded-36340dc6db31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': 45.0,\n",
      " 'movie_genres': array([7]),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'raw_user_age': 46.0,\n",
      " 'timestamp': 879024327,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'138',\n",
      " 'user_occupation_label': 4,\n",
      " 'user_occupation_text': b'doctor',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'53211'}\n",
      "{'bucketized_user_age': 25.0,\n",
      " 'movie_genres': array([ 4, 14]),\n",
      " 'movie_id': b'709',\n",
      " 'movie_title': b'Strictly Ballroom (1992)',\n",
      " 'raw_user_age': 32.0,\n",
      " 'timestamp': 875654590,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'92',\n",
      " 'user_occupation_label': 5,\n",
      " 'user_occupation_text': b'entertainment',\n",
      " 'user_rating': 2.0,\n",
      " 'user_zip_code': b'80525'}\n"
     ]
    }
   ],
   "source": [
    "# Just saying print(ratings) results in the output <PrefetchDataset shapes: ... as above\n",
    "# So the data is extracted from the TensorFlow tensor format using NumPy\n",
    "\n",
    "for x in ratings_raw.take(2).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "73b20e58-b9a5-4997-9044-c6a99b88c9a9",
     "kernelId": ""
    },
    "id": "Eo1v4Oac63Te"
   },
   "source": [
    "TensorFlow's `.map()` applies the function in the brackets to each element of a dataset, so with the lambda inline Python function we can reduce the data down to just the columns that we are going to use. Currently these are the movie title, the time of viewing, the user who viewed it, and their rating of the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "de74ac2e-cca6-4c0a-82a9-39e029e2ace7",
     "kernelId": ""
    },
    "id": "fPH5ffpdw0RR"
   },
   "outputs": [],
   "source": [
    "ratings = ratings_raw.map(lambda x: {\n",
    "    'movie_title': x['movie_title'],\n",
    "    'timestamp': x['timestamp'],\n",
    "    'user_id': x['user_id'],\n",
    "    'user_rating': x['user_rating']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "9bb019fa-669d-4d9b-8618-4474612a4eef",
     "kernelId": ""
    },
    "id": "Iu4XSa_G1nyN"
   },
   "source": [
    "We perform the canonical 80:20 random split of 80% training data and 20% testing data, with in turn 20% of the training data being used for model validation.\n",
    "\n",
    "As mentioned above, we want to avoid leakage of information from the testing set into the training and validation sets, so we split by timestamp. The testing data comes from later times than the training & validation data. Currently the timestamp used is still absolute rather than cyclic (day of week, month of year, etc.), but that would likely just dampen its utility as a feature, so long as the model is not overfitting to it.\n",
    "\n",
    "TensorFlow allows global and local random seeds to be set, which can ensure, e.g., that we get the same 80:20 split each time here.\n",
    "\n",
    "However, randomness remains in the machine learning models when they are run, which is harder to remove. We have not attempted to do so in this project so results may vary when the notebook is rerun, but they should show the same overall pattern, i.e., they are statistically reproducible even if not exactly reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "48387436-5687-4085-af88-8a6b4fcf48bd",
     "kernelId": ""
    },
    "id": "jjyIcUVEMmmP"
   },
   "outputs": [],
   "source": [
    "# Get times as a list so we can get the minimum and maximum time values\n",
    "# In the original data they are dictionaries, so a list is needed\n",
    "# Following the TFRS deep model tutorial (https://www.tensorflow.org/recommenders/examples/deep_recommenders)\n",
    "# a quick way to do this is\n",
    "\n",
    "timestamps = np.concatenate(list(ratings.map(lambda x: x['timestamp']).batch(100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "id": "a29e2097-97ef-4472-a6db-f45c19adcabe",
     "kernelId": ""
    },
    "id": "MD5dhCphMm2I",
    "outputId": "3c10d169-d485-44f7-9018-308fc8e68501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum time value = 874724710\n",
      "Maximum time value = 893286638\n"
     ]
    }
   ],
   "source": [
    "# Get minimum and maximum time values\n",
    "\n",
    "max_time = timestamps.max()\n",
    "min_time = timestamps.min()\n",
    "\n",
    "print('Minimum time value = {}'.format(min_time))\n",
    "print('Maximum time value = {}'.format(max_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "id": "bdbcfdf8-d5dd-48a1-9c43-fea0ef95d11a",
     "kernelId": ""
    },
    "id": "BE16LGI8Mxry",
    "outputId": "0db255df-29ad-428c-8ea3-c5257029b1bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60th percentile time = 885861866.8\n",
      "80th percentile time = 889574252.4\n"
     ]
    }
   ],
   "source": [
    "# Get 60th & 80th percentile times\n",
    "\n",
    "sixtieth_percentile = min_time + 0.6*(max_time - min_time)\n",
    "eightieth_percentile = min_time + 0.8*(max_time - min_time)\n",
    "\n",
    "print('60th percentile time = {}'.format(sixtieth_percentile))\n",
    "print('80th percentile time = {}'.format(eightieth_percentile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "a560dec8-e102-4b2c-826f-44a4810be5f4",
     "kernelId": ""
    },
    "id": "ayX_o1YkMxx6"
   },
   "outputs": [],
   "source": [
    "# Filter original data so that\n",
    "\n",
    "# Training set <= 60th percentile time\n",
    "# 60th < Validation set <= 80th\n",
    "# Testing set > 80th\n",
    "\n",
    "train =      ratings.filter(lambda x: x['timestamp'] <= sixtieth_percentile)\n",
    "validation = ratings.filter(lambda x: x['timestamp'] > sixtieth_percentile and x['timestamp'] <= eightieth_percentile)\n",
    "test =       ratings.filter(lambda x: x['timestamp'] > eightieth_percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "id": "e2da07fd-01b4-46c3-885c-8e77d2aa3bf6",
     "kernelId": ""
    },
    "id": "Jwlwp7-MMx03",
    "outputId": "2c6df211-27f6-40e6-c891-aac0428ddd96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set = 65336\n",
      "Number of rows in validation set = 15778\n",
      "Number of rows in testing set = 18886\n",
      "Total number of rows = 100000\n"
     ]
    }
   ],
   "source": [
    "# We are splitting on time percentiles and not row percentiles,\n",
    "# which means that we have assumed the times are roughly evenly distributed\n",
    "\n",
    "# So check the number of rows in the training, validation, and testing sets is as expected\n",
    "\n",
    "# We could also do row percentiles, which would involve sorting the data by time then shuffling after splitting\n",
    "# As written, the counting is a little slow, but .__len__() doesn't work on the FilterDataset from above\n",
    "\n",
    "# The lengths sum to the number of rows in the original data, currently 100,000, as expected\n",
    "\n",
    "ntimes_tr = 0\n",
    "ntimes_va = 0\n",